<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebSocket Audio Test - Simple Debug</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 900px;
            margin: 20px auto;
            padding: 20px;
            background: #1a1a1a;
            color: #e0e0e0;
        }
        .container {
            background: #2d2d2d;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }
        h1 {
            color: #4CAF50;
            margin-top: 0;
            text-align: center;
        }
        .status-box {
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            font-weight: bold;
            text-align: center;
            font-size: 18px;
        }
        .status.disconnected { background: #f44336; color: white; }
        .status.connected { background: #4CAF50; color: white; }
        .status.recording { background: #FF9800; color: white; }
        
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .stat-box {
            background: #3d3d3d;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }
        .stat-label {
            font-size: 12px;
            color: #aaa;
            margin-bottom: 5px;
        }
        .stat-value {
            font-size: 24px;
            font-weight: bold;
            color: #4CAF50;
        }
        
        button {
            background: #2196F3;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            margin: 5px;
            width: 100%;
            transition: background 0.3s;
        }
        button:hover:not(:disabled) {
            background: #1976D2;
        }
        button:disabled {
            background: #555;
            cursor: not-allowed;
        }
        button.danger {
            background: #f44336;
        }
        button.danger:hover:not(:disabled) {
            background: #d32f2f;
        }
        
        .log {
            background: #1a1a1a;
            border: 2px solid #3d3d3d;
            border-radius: 8px;
            padding: 15px;
            height: 300px;
            overflow-y: auto;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            margin-top: 20px;
        }
        .log-entry {
            margin: 3px 0;
            padding: 2px 0;
        }
        .log-entry.error { color: #f44336; }
        .log-entry.success { color: #4CAF50; }
        .log-entry.info { color: #2196F3; }
        .log-entry.warning { color: #FF9800; }
        .log-entry.audio { color: #9C27B0; }
        
        .audio-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse 1s infinite;
        }
        .audio-indicator.sending { background: #2196F3; }
        .audio-indicator.receiving { background: #4CAF50; }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.3; }
        }
        .audio-level {
            height: 20px;
            background: #3d3d3d;
            border-radius: 10px;
            margin: 10px 0;
            overflow: hidden;
            position: relative;
        }
        .audio-level-bar {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #FF9800, #f44336);
            width: 0%;
            transition: width 0.1s;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ WebSocket Audio Test - Debug Mode</h1>
        
        <div id="status" class="status-box status disconnected">
            ‚ö´ Disconnected
        </div>
        
        <div class="stats">
            <div class="stat-box">
                <div class="stat-label">Audio Sent</div>
                <div class="stat-value" id="sentBytes">0 B</div>
            </div>
            <div class="stat-box">
                <div class="stat-label">Audio Received</div>
                <div class="stat-value" id="receivedBytes">0 B</div>
            </div>
            <div class="stat-box">
                <div class="stat-label">Messages</div>
                <div class="stat-value" id="messageCount">0</div>
            </div>
            <div class="stat-box">
                <div class="stat-label">Connection</div>
                <div class="stat-value" id="connectionState">-</div>
            </div>
        </div>
        
        <div style="margin: 20px 0;">
            <div style="font-size: 12px; color: #aaa; margin-bottom: 5px;">Microphone Level</div>
            <div class="audio-level">
                <div class="audio-level-bar" id="audioLevelBar"></div>
            </div>
        </div>
        
        <div>
            <button id="connectBtn" onclick="connect()">üîå Connect & Start Call</button>
            <button id="disconnectBtn" class="danger" onclick="disconnect()" disabled>‚ùå End Call</button>
        </div>
        
        <div class="log" id="log"></div>
    </div>

    <script>
        let ws = null;
        let audioContext = null;
        let playbackContext = null;
        let isRecording = false;
        let audioStream = null;
        let audioProcessor = null;
        let mediaStreamSource = null;
        
        // Audio playback queue
        let audioQueue = [];
        let isPlayingAudio = false;
        let audioQueueSource = null;
        let audioBufferRemainder = null; // Buffer for odd-byte chunks
        
        // Stats
        let stats = {
            sentBytes: 0,
            receivedBytes: 0,
            messageCount: 0
        };
        
        const WS_URL = 'ws://localhost:8080';
        const SAMPLE_RATE = 24000;  // OpenAI expects 24kHz input
        const OUTPUT_SAMPLE_RATE = 24000;  // OpenAI outputs 24kHz
        
        // Pre-buffer settings for smooth playback
        const PRE_BUFFER_MS = 200; // Wait for 200ms of audio before starting playback
        const BYTES_PER_MS = (OUTPUT_SAMPLE_RATE * 2) / 1000; // 24kHz * 2 bytes per sample / 1000ms = 48 bytes/ms
        const PRE_BUFFER_BYTES = PRE_BUFFER_MS * BYTES_PER_MS; // ~9600 bytes for 200ms
        
        function log(message, type = 'info') {
            const logDiv = document.getElementById('log');
            const entry = document.createElement('div');
            entry.className = `log-entry ${type}`;
            const time = new Date().toLocaleTimeString();
            entry.textContent = `[${time}] ${message}`;
            logDiv.appendChild(entry);
            logDiv.scrollTop = logDiv.scrollHeight;
            
            // Keep only last 100 entries
            while (logDiv.children.length > 100) {
                logDiv.removeChild(logDiv.firstChild);
            }
        }
        
        function updateStatus(status, className) {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = status;
            statusDiv.className = `status-box status ${className}`;
        }
        
        function updateStats() {
            document.getElementById('sentBytes').textContent = formatBytes(stats.sentBytes);
            document.getElementById('receivedBytes').textContent = formatBytes(stats.receivedBytes);
            document.getElementById('messageCount').textContent = stats.messageCount;
            document.getElementById('connectionState').textContent = ws ? ws.readyState : '-';
        }
        
        function formatBytes(bytes) {
            if (bytes < 1024) return bytes + ' B';
            if (bytes < 1024 * 1024) return (bytes / 1024).toFixed(1) + ' KB';
            return (bytes / (1024 * 1024)).toFixed(1) + ' MB';
        }
        
        async function connect() {
            try {
                log('üîå Connecting to WebSocket server...', 'info');
                ws = new WebSocket(WS_URL);
                
                // Set binary type to arraybuffer for better performance
                ws.binaryType = 'arraybuffer';
                
                ws.onopen = async () => {
                    log('‚úÖ Connected to server', 'success');
                    updateStatus('üü¢ Connected - Starting call...', 'connected');
                    document.getElementById('connectBtn').disabled = true;
                    document.getElementById('disconnectBtn').disabled = false;
                    
                    // Automatically start recording
                    await startRecording();
                };
                
                ws.onmessage = async (event) => {
                    stats.messageCount++;
                    
                    if (event.data instanceof Blob) {
                        const audioData = await event.data.arrayBuffer();
                        stats.receivedBytes += audioData.byteLength;
                        log(`üîä Received audio (Blob): ${audioData.byteLength} bytes`, 'audio');
                        playAudio(audioData);
                        updateStats();
                    } else if (event.data instanceof ArrayBuffer) {
                        stats.receivedBytes += event.data.byteLength;
                        const samples = event.data.byteLength / 2; // 16-bit = 2 bytes per sample
                        const durationMs = (samples / OUTPUT_SAMPLE_RATE) * 1000;
                        log(`üîä Received audio (ArrayBuffer): ${event.data.byteLength} bytes (${samples} samples, ~${durationMs.toFixed(1)}ms)`, 'audio');
                        playAudio(event.data);
                        updateStats();
                    } else if (typeof event.data === 'string') {
                        try {
                            const data = JSON.parse(event.data);
                            if (data.type === 'connected') {
                                log(`üì® ${data.message}`, 'success');
                            } else if (data.type === 'flushed') {
                                log(`üì® Audio buffer flushed`, 'info');
                            } else if (data.type === 'interrupt') {
                                log(`üõë ${data.message}`, 'warning');
                                // Clear audio queue and stop current playback
                                clearAudioQueue();
                            } else {
                                log(`üì® JSON: ${JSON.stringify(data)}`, 'info');
                            }
                        } catch (e) {
                            log(`üì® Text message: ${event.data}`, 'info');
                        }
                        updateStats();
                    } else {
                        log(`üì® Unknown message type: ${typeof event.data}`, 'warning');
                        updateStats();
                    }
                };
                
                ws.onerror = (error) => {
                    log(`‚ùå WebSocket error: ${error}`, 'error');
                    updateStatus('‚ùå Connection Error', 'disconnected');
                };
                
                ws.onclose = (event) => {
                    log(`üîå Connection closed (code: ${event.code}, reason: ${event.reason || 'none'})`, 'warning');
                    updateStatus('‚ö´ Disconnected', 'disconnected');
                    document.getElementById('connectBtn').disabled = false;
                    document.getElementById('disconnectBtn').disabled = true;
                    if (isRecording) {
                        stopRecording();
                    }
                    updateStats();
                };
                
            } catch (error) {
                log(`‚ùå Connection error: ${error.message}`, 'error');
            }
        }
        
        function disconnect() {
            if (ws) {
                ws.close();
                ws = null;
            }
            if (isRecording) {
                stopRecording();
            }
        }
        
        async function startRecording() {
            try {
                log('üé§ Requesting microphone access...', 'info');
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: SAMPLE_RATE
                    }
                });
                log('‚úÖ Microphone access granted', 'success');
                
                // Create AudioContext with the stream's sample rate
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const actualSampleRate = audioContext.sampleRate;
                log(`üìä AudioContext sample rate: ${actualSampleRate}Hz`, 'info');
                
                // Don't specify sampleRate for playback - use browser default
                // We'll handle resampling in playAudio function
                playbackContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Resume contexts (required by some browsers)
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                    log('‚ñ∂Ô∏è AudioContext resumed', 'info');
                }
                if (playbackContext.state === 'suspended') {
                    await playbackContext.resume();
                }
                
                // Create MediaStreamSource
                mediaStreamSource = audioContext.createMediaStreamSource(audioStream);
                log(`üìä MediaStreamSource created, stream active: ${audioStream.active}`, 'info');
                
                // Use ScriptProcessorNode (deprecated but widely supported)
                // Buffer size: 4096 samples
                const bufferSize = 4096;
                audioProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
                log(`üìä ScriptProcessor created with buffer size: ${bufferSize}`, 'info');
                
                let audioChunkCount = 0;
                let totalSamples = 0;
                let processorCalled = false;
                
                audioProcessor.onaudioprocess = (e) => {
                    if (!processorCalled) {
                        processorCalled = true;
                        log('‚úÖ AudioProcessor is being called!', 'success');
                    }
                    
                    if (!isRecording) {
                        return;
                    }
                    
                    const inputData = e.inputBuffer.getChannelData(0);
                    totalSamples += inputData.length;
                    
                    // Calculate audio level for visualization
                    let maxLevel = 0;
                    let sumSquares = 0;
                    for (let i = 0; i < inputData.length; i++) {
                        const level = Math.abs(inputData[i]);
                        sumSquares += inputData[i] * inputData[i];
                        if (level > maxLevel) {
                            maxLevel = level;
                        }
                    }
                    
                    // Calculate RMS (Root Mean Square) for better visualization
                    const rms = Math.sqrt(sumSquares / inputData.length);
                    const levelPercent = Math.min(100, rms * 200); // Scale for visibility
                    document.getElementById('audioLevelBar').style.width = levelPercent + '%';
                    
                    // Log occasionally to show processor is working
                    if (audioChunkCount % 100 === 0 && audioChunkCount > 0) {
                        log(`üìä Audio level: ${(rms * 100).toFixed(2)}% (max: ${(maxLevel * 100).toFixed(2)}%)`, 'info');
                    }
                    
                    if (!ws || ws.readyState !== WebSocket.OPEN) {
                        return;
                    }
                    
                    // Resample if needed (AudioContext might be 48kHz, we need 24kHz)
                    let processedData = inputData;
                    if (actualSampleRate !== SAMPLE_RATE) {
                        // Simple downsampling: take every Nth sample
                        const ratio = actualSampleRate / SAMPLE_RATE;
                        const downsampledLength = Math.floor(inputData.length / ratio);
                        processedData = new Float32Array(downsampledLength);
                        for (let i = 0; i < downsampledLength; i++) {
                            processedData[i] = inputData[Math.floor(i * ratio)];
                        }
                    }
                    
                    // Convert Float32 to Int16 PCM
                    const pcm16 = new Int16Array(processedData.length);
                    for (let i = 0; i < processedData.length; i++) {
                        const s = Math.max(-1, Math.min(1, processedData[i]));
                        pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }
                    
                    try {
                        ws.send(pcm16.buffer);
                        stats.sentBytes += pcm16.byteLength;
                        audioChunkCount++;
                        
                        // Log every 50 chunks (roughly every 2-3 seconds)
                        if (audioChunkCount % 50 === 0) {
                            log(`üì§ Sent ${audioChunkCount} chunks, ${stats.sentBytes} bytes`, 'info');
                        }
                        
                        updateStats();
                    } catch (err) {
                        log(`‚ö†Ô∏è Error sending audio: ${err.message}`, 'error');
                    }
                };
                
                // Connect the audio processing chain
                // IMPORTANT: ScriptProcessor needs to be connected to destination to work!
                mediaStreamSource.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                log('üìä Audio processing chain connected', 'info');
                
                isRecording = true;
                updateStatus('üìû Call Active - Speak now!', 'recording');
                log('üé§ Recording started - speak into your microphone!', 'success');
                log(`üìä Processing audio at ${actualSampleRate}Hz, sending at ${SAMPLE_RATE}Hz (OpenAI expects 24kHz)`, 'info');
                log('üí° If audio level stays at 0, check microphone permissions and try speaking louder', 'info');
                
                // Check if processor is called after a short delay
                setTimeout(() => {
                    if (!processorCalled) {
                        log('‚ö†Ô∏è AudioProcessor not called after 1 second - check microphone connection', 'warning');
                    }
                }, 1000);
                
            } catch (error) {
                log(`‚ùå Error starting recording: ${error.message}`, 'error');
                log(`‚ùå Full error: ${JSON.stringify(error)}`, 'error');
                updateStatus(`‚ùå Error: ${error.message}`, 'disconnected');
            }
        }
        
        function stopRecording() {
            isRecording = false;
            
            if (audioProcessor) {
                audioProcessor.disconnect();
                audioProcessor = null;
            }
            
            if (mediaStreamSource) {
                mediaStreamSource.disconnect();
                mediaStreamSource = null;
            }
            
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            
            if (playbackContext) {
                playbackContext.close();
                playbackContext = null;
            }
            
            // Clear audio queue
            clearAudioQueue();
            
            // Reset audio level bar
            document.getElementById('audioLevelBar').style.width = '0%';
            
            log('üõë Recording stopped', 'info');
        }
        
        function playAudio(audioData) {
            if (!playbackContext || playbackContext.state === 'closed') {
                // Use browser's default sample rate (usually 48kHz)
                playbackContext = new (window.AudioContext || window.webkitAudioContext)();
                log(`üìä Playback context created with sample rate: ${playbackContext.sampleRate}Hz`, 'info');
            }
            
            if (playbackContext.state === 'suspended') {
                playbackContext.resume();
            }
            
            // Handle odd-byte chunks by combining with remainder
            let processedData = audioData;
            if (audioBufferRemainder) {
                // Combine remainder with new data
                const combined = new Uint8Array(audioBufferRemainder.byteLength + audioData.byteLength);
                combined.set(new Uint8Array(audioBufferRemainder), 0);
                combined.set(new Uint8Array(audioData), audioBufferRemainder.byteLength);
                processedData = combined.buffer;
                audioBufferRemainder = null;
            }
            
            // Check if processed data has odd number of bytes
            if (processedData.byteLength % 2 !== 0) {
                // Store the last byte for next chunk
                const lastByte = new Uint8Array(processedData).slice(-1);
                audioBufferRemainder = lastByte.buffer;
                // Use all but the last byte
                processedData = processedData.slice(0, processedData.byteLength - 1);
            }
            
            // Only add to queue if we have valid data (even number of bytes)
            if (processedData.byteLength > 0) {
                audioQueue.push(processedData);
            }
            
            // Calculate total buffered bytes
            let totalBufferedBytes = 0;
            for (let i = 0; i < audioQueue.length; i++) {
                totalBufferedBytes += audioQueue[i].byteLength;
            }
            if (audioBufferRemainder) {
                totalBufferedBytes += audioBufferRemainder.byteLength;
            }
            
            // Start playing if not already playing AND we have enough pre-buffered data
            if (!isPlayingAudio) {
                if (totalBufferedBytes >= PRE_BUFFER_BYTES) {
                    const bufferedMs = totalBufferedBytes / BYTES_PER_MS;
                    log(`üì¶ Pre-buffer ready: ${bufferedMs.toFixed(1)}ms (${totalBufferedBytes} bytes), starting playback`, 'info');
                    processAudioQueue();
                } else {
                    const bufferedMs = totalBufferedBytes / BYTES_PER_MS;
                    log(`‚è≥ Buffering: ${bufferedMs.toFixed(1)}ms / ${PRE_BUFFER_MS}ms (${totalBufferedBytes} / ${PRE_BUFFER_BYTES} bytes)`, 'info');
                }
            }
        }
        
        function clearAudioQueue() {
            // Clear the queue
            audioQueue = [];
            audioBufferRemainder = null;
            
            // Stop current playback if any
            if (audioQueueSource) {
                try {
                    audioQueueSource.stop();
                } catch (e) {
                    // Source might already be stopped
                }
                audioQueueSource = null;
            }
            
            isPlayingAudio = false;
            log('üõë Audio queue cleared due to interrupt', 'warning');
        }
        
        async function processAudioQueue() {
            if (isPlayingAudio) {
                return;
            }
            
            isPlayingAudio = true;
            
            while (audioQueue.length > 0) {
                const audioData = audioQueue.shift();
                
                if (!audioData || audioData.byteLength === 0) {
                    log('‚ö†Ô∏è Skipping empty audio chunk', 'warning');
                    continue;
                }
                
                try {
                    // Ensure we have an even number of bytes
                    if (audioData.byteLength % 2 !== 0) {
                        log(`‚ö†Ô∏è Skipping chunk with odd byte count: ${audioData.byteLength} bytes`, 'warning');
                        continue;
                    }
                    
                    const int16Data = new Int16Array(audioData);
                    const samples = int16Data.length;
                    const durationMs = (samples / OUTPUT_SAMPLE_RATE) * 1000;
                    
                    log(`üéµ Playing audio chunk: ${samples} samples (~${durationMs.toFixed(1)}ms)`, 'audio');
                    
                    // Convert Int16 to Float32
                    const float32Data = new Float32Array(int16Data.length);
                    for (let i = 0; i < int16Data.length; i++) {
                        // Normalize to [-1, 1]
                        float32Data[i] = Math.max(-1, Math.min(1, int16Data[i] / 32768.0));
                    }
                    
                    // Create buffer at OpenAI's output sample rate (24kHz)
                    // The browser will automatically resample to playbackSampleRate (usually 48kHz)
                    const buffer = playbackContext.createBuffer(1, float32Data.length, OUTPUT_SAMPLE_RATE);
                    buffer.getChannelData(0).set(float32Data);
                    
                    // Create and play source
                    const source = playbackContext.createBufferSource();
                    source.buffer = buffer;
                    source.connect(playbackContext.destination);
                    audioQueueSource = source; // Store reference for potential interruption
                    
                    // Wait for this buffer to finish before playing next
                    await new Promise((resolve, reject) => {
                        let resolved = false;
                        const timeout = setTimeout(() => {
                            if (!resolved) {
                                resolved = true;
                                log(`‚ö†Ô∏è Audio chunk timeout after ${(durationMs + 100).toFixed(0)}ms`, 'warning');
                                resolve();
                            }
                        }, durationMs + 100); // Add 100ms buffer
                        
                        source.onended = () => {
                            if (!resolved) {
                                resolved = true;
                                clearTimeout(timeout);
                                audioQueueSource = null;
                                log(`‚úÖ Audio chunk finished playing`, 'audio');
                                resolve();
                            }
                        };
                        
                        source.onerror = (error) => {
                            if (!resolved) {
                                resolved = true;
                                clearTimeout(timeout);
                                log(`‚ùå Audio playback error: ${error}`, 'error');
                                reject(error);
                            }
                        };
                        
                        try {
                            source.start(0);
                            log(`‚ñ∂Ô∏è Started playing audio chunk`, 'audio');
                        } catch (error) {
                            if (!resolved) {
                                resolved = true;
                                clearTimeout(timeout);
                                log(`‚ùå Error starting audio: ${error.message}`, 'error');
                                reject(error);
                            }
                        }
                    });
                    
                } catch (error) {
                    log(`‚ùå Error processing audio chunk: ${error.message}`, 'error');
                }
            }
            
            isPlayingAudio = false;
            audioQueueSource = null;
            log(`üèÅ Audio queue finished, waiting for more chunks...`, 'info');
            
            // If new chunks arrived while we were processing, check if we have enough buffer
            if (audioQueue.length > 0) {
                let totalBufferedBytes = 0;
                for (let i = 0; i < audioQueue.length; i++) {
                    totalBufferedBytes += audioQueue[i].byteLength;
                }
                if (audioBufferRemainder) {
                    totalBufferedBytes += audioBufferRemainder.byteLength;
                }
                
                if (totalBufferedBytes >= PRE_BUFFER_BYTES) {
                    log(`üì• New chunks arrived with enough buffer, continuing playback...`, 'info');
                    processAudioQueue();
                } else {
                    const bufferedMs = totalBufferedBytes / BYTES_PER_MS;
                    log(`üì• New chunks arrived, but need more buffer: ${bufferedMs.toFixed(1)}ms / ${PRE_BUFFER_MS}ms`, 'info');
                }
            }
        }
        
        // Update stats every second
        setInterval(updateStats, 1000);
    </script>
</body>
</html>

